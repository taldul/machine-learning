{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b69a810",
   "metadata": {},
   "source": [
    "# <font color=blue> <u>  HomeWork 1</u> </font>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5fe04",
   "metadata": {},
   "source": [
    "## ID1:205914609\n",
    "\n",
    "## ID2:207906884\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a822ef",
   "metadata": {},
   "source": [
    "## <u> Classification Tree </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85536ea",
   "metadata": {},
   "source": [
    "Import relevant packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7b473",
   "metadata": {},
   "source": [
    "##  Section A+B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e91cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "import random\n",
    "import bisect\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a545c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "    \n",
    "Function to calculate Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fb0775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(classffier_arr: np.ndarray):\n",
    "    # determine which unique values the classffier_arry have and count how many from each class\n",
    "    values, counts = np.unique(classffier_arr, return_counts=True)\n",
    "    # calculate how many values all and all in the classfier\n",
    "    S = len(classffier_arr)\n",
    "    # calculate gini index equation\n",
    "    gini_idx = 1 - ((counts / S) ** 2).sum()\n",
    "\n",
    "    return gini_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345f679",
   "metadata": {},
   "source": [
    "Function to calculate the total Gini of the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ef4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_gini(attr_array: np.ndarray, splitter: float, classffier_arr: np.ndarray):\n",
    "    # store all values above the splitter value\n",
    "    above_splitter = classffier_arr[attr_array >= splitter]\n",
    "    # store all values below the splitter value\n",
    "    below_splitter = classffier_arr[attr_array < splitter]\n",
    "\n",
    "    # calculate gini index for both groups\n",
    "    above_splitter_gini = gini_index(classffier_arr=above_splitter)\n",
    "    below_splitter_gini = gini_index(classffier_arr=below_splitter)\n",
    "    # calculate how many values in the column\n",
    "    S = len(classffier_arr)\n",
    "    # calculate the totla gini of the column, based on the current splitter\n",
    "    total = (len(above_splitter) / S) * above_splitter_gini + (len(below_splitter) / S) * below_splitter_gini\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ed2fb",
   "metadata": {},
   "source": [
    "Function to calculate the best Gini Index for an attribute\n",
    "<br>\n",
    "Returns the best Gini and the best splitter of the obserations to get this Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fac3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_gini_for_attr(attr_array, classfier_arr):\n",
    "    # store the unique values of the data in a sorted array\n",
    "    unique_attr_array = np.unique(attr_array)\n",
    "    min_gini = 1\n",
    "    min_splitter = 0\n",
    "\n",
    "    # for every mean between 2 adjacent values, calculate total gini\n",
    "    for i in range((len(unique_attr_array) - 1)):\n",
    "        splitter = float(np.mean(unique_attr_array[i:i + 2]))\n",
    "        temp_gini = total_gini(attr_array, splitter, classfier_arr)\n",
    "\n",
    "        # if its the best total gini store it as min_gini\n",
    "        if temp_gini < min_gini:\n",
    "            min_gini = temp_gini\n",
    "            min_splitter = splitter\n",
    "\n",
    "    return min_gini, min_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6062238",
   "metadata": {},
   "source": [
    "Function to choose the best attribute to split by the data\n",
    "<br>\n",
    "The attribute with the minimum Gini index will be chosen\n",
    "<br>\n",
    "The function returns:\n",
    "<br>\n",
    "min_gini- the minimum Gini of the attribute to split by\n",
    "<br>\n",
    "splitter-the value to split by the data\n",
    "<br>\n",
    "best_col-the name to the attribute to split the data by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b43b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_column_to_split_by(data_df: pd.DataFrame, classffier_arr: np.ndarray):\n",
    "    best_col = ''\n",
    "    splitter = 0\n",
    "    min_gini = 1.1\n",
    "    # for every attribute in the data frame\n",
    "    for column in data_df.columns.values:\n",
    "        # calculate the best gini for the current attribute\n",
    "        temp_gini, temp_splitter = best_gini_for_attr(data_df[column].values, classfier_arr=classffier_arr)\n",
    "        # if it is the best total gini-store it as min gini,\n",
    "        # best splitter for this attribute and the attribute's name\n",
    "        if temp_gini < min_gini:\n",
    "            min_gini = temp_gini\n",
    "            splitter = temp_splitter\n",
    "            best_col = column\n",
    "\n",
    "    return min_gini, splitter, best_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fafe944",
   "metadata": {},
   "source": [
    "Class Node: defining the nodes of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f06f6a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_df: pd.DataFrame,\n",
    "            classffier_arr: np.ndarray,\n",
    "            depth\n",
    "    ):\n",
    "        self.data_df = data_df\n",
    "        self.classffier_arr = classffier_arr\n",
    "        self.depth = depth\n",
    "        self.column = None\n",
    "        self.gini = None\n",
    "        self.splitter = None\n",
    "\n",
    "        # array for left and right children\n",
    "        self.children = []\n",
    "        # determine which unique values the classffier_arry have and count how many from each class\n",
    "        values, counts = np.unique(self.classffier_arr, return_counts=True)\n",
    "        # store and array of the unique values in the classfier\n",
    "        self.values_names = values\n",
    "        # store the count of each class\n",
    "        self.values = counts\n",
    "        # store the class with the most values\n",
    "        self.cls = values[np.argmax(counts)]\n",
    "\n",
    "    # if all the data is form 1 class\n",
    "    def perfectly_classified(self):\n",
    "        return len(self.values) == 1\n",
    "\n",
    "    # Function to predict the class of an observation in the data\n",
    "    # gets only one row from the data\n",
    "    def classify(self, data_row):\n",
    "        # if the node is a leaf than classfiy\n",
    "        # the current row as the majority class of the current node\n",
    "        if len(self.children) == 0:\n",
    "            return self.cls\n",
    "        # store the current value of the column we are currently splitting by\n",
    "        current_value = data_row[self.column]\n",
    "\n",
    "        # if the current value is smaller than the node's splitter\n",
    "        # go to the left child to classify\n",
    "        # return the result of the left child\n",
    "        if current_value < self.splitter and self.children[0] is not None:\n",
    "            return self.children[0].classify(data_row)\n",
    "        # if the current value is bigger or equals to the node's splitter\n",
    "        # go to the right child to classify\n",
    "        # return the result of the right child\n",
    "        elif current_value >= self.splitter and self.children[1] is not None:\n",
    "            return self.children[1].classify(data_row)\n",
    "        else:\n",
    "            # return the class of this node\n",
    "            return self.cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ca1a6",
   "metadata": {},
   "source": [
    "Class dt_tree: the class of the classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9554b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dt_tree:\n",
    "    def __init__(self, depth\n",
    "                 ):\n",
    "        self.root = None\n",
    "        self.depth = depth\n",
    "\n",
    "    def dt_trainer(self, data_df: pd.DataFrame, classffier_arr: np.ndarray):\n",
    "        # Initialize a queue for the nodes of the tree\n",
    "        q_nodes: List[Node] = []\n",
    "        # create root node\n",
    "        root_node = Node(data_df=data_df, classffier_arr=classffier_arr, depth=0)\n",
    "        # add root node to the queue\n",
    "        q_nodes.append(root_node)\n",
    "        # as long as we have nodes in the queue\n",
    "        while len(q_nodes) > 0:\n",
    "            # get first node in the queue\n",
    "            node = q_nodes.pop(0)\n",
    "            # check if the node is a leaf(contains only one class) or data frame is empty\n",
    "            if node.perfectly_classified() or len(node.data_df.columns) == 0 or node.depth == self.depth:\n",
    "                # calculate gini score for the current data\n",
    "                node.gini = gini_index(node.classffier_arr)\n",
    "                # continue to the next node\n",
    "                continue\n",
    "            # calculate which attribute to split by-the one with the best gini will be chosen\n",
    "            min_gini, splitter, best_col = choose_column_to_split_by(data_df=node.data_df,\n",
    "                                                                     classffier_arr=node.classffier_arr)\n",
    "\n",
    "            # store the values of the attribute to split data by\n",
    "            node.column = best_col\n",
    "            node.gini = min_gini\n",
    "            node.splitter = splitter\n",
    "\n",
    "            # Store the data that has bigger value from the splitter\n",
    "            # and drop the current attribute from the data\n",
    "            left_node_df = node.data_df[node.data_df[best_col] < splitter]\n",
    "            left_node = None\n",
    "            # if exists data for the left node(bigger than the splitter)\n",
    "            if len(left_node_df) > 0:\n",
    "                # create left node and store the data in it\n",
    "                left_node = Node(\n",
    "                    data_df=left_node_df,\n",
    "                    classffier_arr=node.classffier_arr[node.data_df[best_col] < splitter],\n",
    "                    depth=node.depth + 1\n",
    "                )\n",
    "                # add the new node to the queue\n",
    "                q_nodes.append(left_node)\n",
    "\n",
    "            # Store the  data that has smaller value from the splitter\n",
    "            # and drop the current attribute from the data\n",
    "            right_node_df = node.data_df[node.data_df[best_col] >= splitter]\n",
    "            right_node = None\n",
    "            # if exists data for the right node(smaller than the splitter)\n",
    "            if len(right_node_df) > 0:\n",
    "                # create right node and store the data in it\n",
    "                right_node = Node(\n",
    "                    data_df=node.data_df[node.data_df[best_col] >= splitter],\n",
    "                    classffier_arr=node.classffier_arr[node.data_df[best_col] >= splitter],\n",
    "                    depth=node.depth + 1\n",
    "                )\n",
    "                # add the new node to the queue\n",
    "                q_nodes.append(right_node)\n",
    "            # store the left and right new nodes to the current node's children array\n",
    "            node.children = [left_node, right_node]\n",
    "\n",
    "        return root_node\n",
    "\n",
    "    def dt_inference(self, dt: Node, data_df: pd.DataFrame, calssifier_arr):\n",
    "        correct = 0\n",
    "        # as long as we have rows in the data frame\n",
    "        for i in range(len(data_df)):\n",
    "            row = data_df.iloc[i]\n",
    "            # predict the class of the current row according to the tree's decision\n",
    "            predicted = dt.classify(row)\n",
    "            # store the real class of the current row\n",
    "            real = calssifier_arr[i]\n",
    "\n",
    "            # if the predition was corret- count it\n",
    "            if real == predicted:\n",
    "                correct += 1\n",
    "\n",
    "        # calculate how many true predictions where made from all the data\n",
    "        accuracy = correct / len(data_df)\n",
    "        return accuracy\n",
    "\n",
    "    def dt_results(self, dt: Node, data_df: pd.DataFrame):\n",
    "        results_arr = []\n",
    "\n",
    "        # as long as we have rows in the data frame\n",
    "        for i in range(len(data_df)):\n",
    "            row = data_df.iloc[i]\n",
    "            # predict the class of the current row according to the tree's decision\n",
    "            predicted = dt.classify(row)\n",
    "            results_arr.append(predicted)\n",
    "\n",
    "        return results_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46820db9",
   "metadata": {},
   "source": [
    "Function to choose the tree with the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e42098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_class_tree(class_tree_A, class_tree_B, class_tree_C, accA, accB, accC):\n",
    "    if accA >= accB and accA >= accC:\n",
    "        return class_tree_A, accA\n",
    "    if accB >= accA and accB >= accC:\n",
    "        return class_tree_B, accB\n",
    "    else:\n",
    "        return class_tree_C, accC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a69defc",
   "metadata": {},
   "source": [
    "Train, validate and test the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5902fed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n",
      "Tree with 5 was built successfully!\n",
      "The accuracy of the tree is: 0.9039323046291687\n",
      "Tree with 10 was built successfully!\n",
      "The accuracy of the tree is: 0.9069188651070185\n",
      "Tree with 15 was built successfully!\n",
      "The accuracy of the tree is: 0.9024390243902439\n",
      "The best Classification Tree has 10 depth, with accuracy: 90.692%\n",
      "run time: 0:01:40.858751\n",
      "Test:\n",
      "The accuracy of the tree is: 90.804%\n"
     ]
    }
   ],
   "source": [
    "# get the path to the folder of the script\n",
    "folder_path = os.getcwd()\n",
    "# store csv file name as a string\n",
    "csv_name = \"data.csv\"\n",
    "# create the path and store it\n",
    "csv_path = os.path.join(folder_path, csv_name)\n",
    "# Read data from csv file and store in real_estate\n",
    "real_estate = pd.read_csv(csv_path, index_col=[0])\n",
    "                             \n",
    "# store area_type column as classfier\n",
    "classfier_arr = np.array(real_estate['area_type'])\n",
    " # store data without the classfier\n",
    "real_estate_features = real_estate.drop('area_type', axis=1)\n",
    "# store train data\n",
    "train_df = real_estate_features.iloc[:8040]\n",
    "# store the classfier for train data\n",
    "train_cls_arr = classfier_arr[:8040]\n",
    "# store the validation data\n",
    "validation_df = real_estate_features.iloc[8041:10050]\n",
    "# store the validation classifer\n",
    "validation_cls_arr = classfier_arr[8041:10050]\n",
    "print(\"Validation:\")\n",
    "##############trail 1 with depth 5 ##########\n",
    "start_time_A = datetime.now()\n",
    "# build decision tree with depth 5\n",
    "class_tree_A = dt_tree(5)\n",
    "class_tree_A.root = class_tree_A.dt_trainer(train_df, train_cls_arr)\n",
    "print(\"Tree with\", class_tree_A.depth, \"was built successfully!\")\n",
    "accA = class_tree_A.dt_inference(class_tree_A.root, validation_df, validation_cls_arr)\n",
    "print(\"The accuracy of the tree is:\", accA)\n",
    "end_time_A = datetime.now()\n",
    "##########################################################\n",
    "\n",
    "##############trail 2 with depth 10 ##########\n",
    "start_time_B = datetime.now()\n",
    "# build decision tree with depth 10\n",
    "class_tree_B = dt_tree(10)\n",
    "class_tree_B.root = class_tree_B.dt_trainer(train_df, train_cls_arr)\n",
    "print(\"Tree with\", class_tree_B.depth, \"was built successfully!\")\n",
    "accB = class_tree_B.dt_inference(class_tree_B.root, validation_df, validation_cls_arr)\n",
    "print(\"The accuracy of the tree is:\", accB)\n",
    "end_time_B = datetime.now()\n",
    "##########################################################\n",
    "\n",
    "##############trail 3 with depth 15 ##########\n",
    "start_time_C = datetime.now()\n",
    "# build decision tree with depth 15\n",
    "class_tree_C = dt_tree(15)\n",
    "class_tree_C.root = class_tree_C.dt_trainer(train_df, train_cls_arr)\n",
    "print(\"Tree with\", class_tree_C.depth, \"was built successfully!\")\n",
    "accC = class_tree_C.dt_inference(class_tree_C.root, validation_df, validation_cls_arr)\n",
    "print(\"The accuracy of the tree is:\", accC)\n",
    "end_time_C = datetime.now()\n",
    "##########################################################\n",
    "#check which tree has the best accuracy\n",
    "best_tree, best_accuracy = best_class_tree(class_tree_A, class_tree_B, class_tree_C, accA, accB, accC)\n",
    "best_accuracy = f\"{best_accuracy:.3%}\"\n",
    "print(\"The best Classification Tree has\", best_tree.depth, \"depth, with accuracy:\", best_accuracy)\n",
    "best_runtime=None\n",
    "if best_tree == class_tree_A:\n",
    "    print('run time: {}'.format(end_time_A - start_time_A))\n",
    "    best_runtime=end_time_A - start_time_A\n",
    "if best_tree == class_tree_B:\n",
    "    print('run time: {}'.format(end_time_B - start_time_B))\n",
    "    best_runtime=end_time_B - start_time_B\n",
    "if best_tree == class_tree_C:\n",
    "    print('run time: {}'.format(end_time_C - start_time_C))\n",
    "    best_runtime=end_time_C - start_time_C\n",
    "\n",
    "# store the test data\n",
    "test_df = real_estate_features.iloc[10051:]\n",
    "# store the test classifer\n",
    "test_cls_arr = classfier_arr[10051:]\n",
    "# calculate accuracy of the test data based on the decision tree\n",
    "test_acc = best_tree.dt_inference(best_tree.root, test_df, test_cls_arr)\n",
    "test_acc = f\"{test_acc:.3%}\"\n",
    "print(\"Test:\")\n",
    "print(\"The accuracy of the tree is:\", test_acc)\n",
    "# store test results\n",
    "test_results = best_tree.dt_results(best_tree.root, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c643314",
   "metadata": {},
   "source": [
    "##  Section C:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0474f6e6",
   "metadata": {},
   "source": [
    "Import relevant packages from sklearn to build a classification tree and regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d80af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "# model building\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# model evaluation\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ee843",
   "metadata": {},
   "source": [
    "define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a3950fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# define parameter grid\n",
    "parameters_grid = {\n",
    "    'max_depth': [5, 10, 15]\n",
    "}\n",
    "\n",
    "# define grid search\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=parameters_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba713476",
   "metadata": {},
   "source": [
    "fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "501487be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=10)\n"
     ]
    }
   ],
   "source": [
    "# fit estimator\n",
    "sk_class_tree_start=datetime.now()\n",
    "grid_search.fit(train_df, train_cls_arr)\n",
    "sk_class_tree_end=datetime.now()\n",
    "\n",
    "# get best estimator\n",
    "best = grid_search.best_estimator_\n",
    "#print best max_depth\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bebc7c",
   "metadata": {},
   "source": [
    "predict the area type for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a98012a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = best.predict(validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca98d63",
   "metadata": {},
   "source": [
    "calculate accuracy for validation data and print it+compare to the accuracy of the presonal decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed81753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklrean tree:\n",
      "                          Accuracy\n",
      "Decision Tree Classifier     0.911\n",
      "run time: 0:00:00.763864\n",
      "personal tree:\n",
      "90.692%\n",
      "run time: 0:01:40.858751\n"
     ]
    }
   ],
   "source": [
    "#calculate accuracy for validation data\n",
    "acc = round(accuracy_score(validation_cls_arr,predictions), 3)\n",
    "\n",
    "df = pd.DataFrame([acc])\n",
    "df = df.rename(index={0: 'Decision Tree Classifier'}, columns={0: 'Accuracy'})\n",
    "print(\"sklrean tree:\")\n",
    "print(df)\n",
    "print('run time: {}'.format(sk_class_tree_end-sk_class_tree_start))\n",
    "print(\"personal tree:\")\n",
    "print(best_accuracy)\n",
    "print('run time: {}'.format(best_runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15da27dd",
   "metadata": {},
   "source": [
    "predict the area type on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8629153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = best.predict(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5772e37e",
   "metadata": {},
   "source": [
    "calculate accuracy for test data and print it+compare to the accuracy of the presonal decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e5fb2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklrean tree:\n",
      "                          Accuracy\n",
      "Decision Tree Classifier     0.911\n",
      "personal tree:\n",
      "90.804%\n"
     ]
    }
   ],
   "source": [
    "#calculate accuracy for validation data\n",
    "acc = round(accuracy_score(test_cls_arr,predictions), 3)\n",
    "\n",
    "df = pd.DataFrame([acc])\n",
    "df = df.rename(index={0: 'Decision Tree Classifier'}, columns={0: 'Accuracy'})\n",
    "print(\"sklrean tree:\")\n",
    "print(df)\n",
    "print(\"personal tree:\")\n",
    "print(test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d47f24",
   "metadata": {},
   "source": [
    "Our decision tree classifier has a very close accuracy to sklrean decision tree classifier,\n",
    "but the sklearn algorithm is much faster.\n",
    "We estimate the reason for that is that we used some while and for loops, while sklearn used more efficient methods we could not think of.\n",
    "In addition they use gridsearch, which makes their algorithm more accurate and much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaacd946",
   "metadata": {},
   "source": [
    "## <u> Regression Tree </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21187b",
   "metadata": {},
   "source": [
    "## Section A+B:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b10a7",
   "metadata": {},
   "source": [
    "Function to calculate the SSR score of an attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11b03f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssr_score(attr_array: np.ndarray, splitter: float, classfier_arr: np.ndarray):\n",
    "    # if the data frame is empty, then the sum is 0\n",
    "    if len(classfier_arr[attr_array >= splitter]) == 0:\n",
    "        sum_above_splitter = 0\n",
    "    else:\n",
    "        # calculate the mean of all values above the splitter value\n",
    "        mean_above_splitter = classfier_arr[attr_array >= splitter].mean()\n",
    "        # sum the square residuals(real attribute value-mean of above split values) of all elements in the array\n",
    "        # that are bigger than the splitter\n",
    "        sum_above_splitter = ((classfier_arr[attr_array >= splitter] - mean_above_splitter) ** 2).sum()\n",
    "    # if the data frame is empty, then the sum is 0\n",
    "    if len(classfier_arr[attr_array < splitter]) == 0:\n",
    "        sum_below_splitter = 0\n",
    "    else:\n",
    "        # calculate the mean of below the splitter value\n",
    "        mean_below_splitter = classfier_arr[attr_array < splitter].mean()\n",
    "        # sum the square residuals(real attribute value-mean of below split values) of all elements in the array\n",
    "        # that are smaller than the splitter\n",
    "        sum_below_splitter = ((classfier_arr[attr_array < splitter] - mean_below_splitter) ** 2).sum()\n",
    "\n",
    "    # return SSR score for this split\n",
    "    return sum_above_splitter + sum_below_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82cc49",
   "metadata": {},
   "source": [
    "Function to calculate the best SSR score for an attribute\n",
    "Returns the best SSR and the best splitter of the obserations to get this SSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db12e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_ssr_for_attr(attr_array, classfier_arr):\n",
    "    # store the unique values of the data in a sorted array\n",
    "    unique_attr_array = np.unique(attr_array)\n",
    "    min_ssr = None\n",
    "    min_splitter = None\n",
    "\n",
    "    if len(unique_attr_array) == 1:\n",
    "        return ssr_score(attr_array, unique_attr_array[0], classfier_arr), unique_attr_array[0]\n",
    "\n",
    "    # for every mean between 2 adjacent values, calculate ssr\n",
    "    for i in range((len(unique_attr_array) - 1)):\n",
    "        temp_splitter = float(np.mean(unique_attr_array[i:i + 2]))\n",
    "        temp_ssr = ssr_score(attr_array, temp_splitter, classfier_arr)\n",
    "\n",
    "        # if it is the first ssr calculated-store it as min ssr\n",
    "        if min_ssr is None:\n",
    "            min_ssr = temp_ssr\n",
    "            min_splitter = temp_splitter\n",
    "        # if its ssr store it as min_ssr\n",
    "        if temp_ssr < min_ssr:\n",
    "            min_ssr = temp_ssr\n",
    "            min_splitter = temp_splitter\n",
    "\n",
    "    return min_ssr, min_splitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b0f191",
   "metadata": {},
   "source": [
    "Function to choose the best attribute to split by the data\n",
    "<br>\n",
    "The attribute with the minimum SSR will be chosen\n",
    "<br>\n",
    "The function returns:\n",
    "<br>\n",
    "min_ssr- the minimum SSR of the attribute to split by\n",
    "<br>\n",
    "splitter-the value to split by the data\n",
    "<br>\n",
    "best_col-the name to the attribute to split the data by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c511950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_column_to_split_by(data_df: pd.DataFrame, classffier_arr: np.ndarray):\n",
    "    best_col = ''\n",
    "    splitter = 0\n",
    "    min_ssr = None\n",
    "    # for every attribute in the data frame\n",
    "    for column in data_df.columns.values:\n",
    "        # calculate the best ssr for the current attribute\n",
    "        temp_ssr, temp_splitter = best_ssr_for_attr(data_df[column].values, classfier_arr=classffier_arr)\n",
    "        # if it is the best ssr-store it as min ssr,\n",
    "        # best splitter for this attribute and the attribute's name\n",
    "        if min_ssr is None:\n",
    "            min_ssr = temp_ssr\n",
    "            splitter = temp_splitter\n",
    "            best_col = column\n",
    "\n",
    "        if temp_ssr < min_ssr:\n",
    "            min_ssr = temp_ssr\n",
    "            splitter = temp_splitter\n",
    "            best_col = column\n",
    "    return min_ssr, splitter, best_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a136630f",
   "metadata": {},
   "source": [
    "Class Node: defining the nodes of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e426972",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_df: pd.DataFrame,\n",
    "            classffier_arr: np.ndarray,\n",
    "            depth\n",
    "    ):\n",
    "        self.data_df = data_df\n",
    "        self.classffier_arr = classffier_arr\n",
    "        self.depth = depth\n",
    "        self.column = None\n",
    "        self.ssr = None\n",
    "        self.splitter = None\n",
    "        # array for left and right children\n",
    "        self.children = []\n",
    "        self.mean_price = self.classffier_arr.mean()\n",
    "\n",
    "    # gets only one row from the data\n",
    "    def classify(self, data_row):\n",
    "        # if the node is a leaf than classfiy\n",
    "        # the current row as the majority class of the current node\n",
    "        if len(self.children) == 0:\n",
    "            return self.mean_price\n",
    "        # store the current value of the column we are currently splitting by\n",
    "        current_value = data_row[self.column]\n",
    "\n",
    "        # if the current value is smaller than the node's splitter\n",
    "        # go to the left child to classify\n",
    "        # return the result of the left child\n",
    "        if current_value < self.splitter and self.children[0] is not None:\n",
    "            return self.children[0].classify(data_row)\n",
    "        # if the current value is bigger or equal than the node's splitter\n",
    "        # go to the right child to classify\n",
    "        # return the result of the right child\n",
    "        elif current_value >= self.splitter and self.children[1] is not None:\n",
    "            return self.children[1].classify(data_row)\n",
    "        else:\n",
    "            # return the class of this node\n",
    "            return self.mean_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6be66",
   "metadata": {},
   "source": [
    "Class Regression_tree: defining the Regressio tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6706414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_tree:\n",
    "    def __init__(self,\n",
    "                 min_split_size, depth):\n",
    "        self.depth = depth\n",
    "        self.min_split_size = min_split_size\n",
    "        self.root = None\n",
    "\n",
    "    def dt_trainer(self, data_df: pd.DataFrame, classffier_arr: np.ndarray):\n",
    "        # Initialize a queue for the nodes of the tree\n",
    "        q_nodes: List[Node] = []\n",
    "        # create root node\n",
    "        self.root = Node(data_df=data_df, classffier_arr=classffier_arr, depth=0)\n",
    "        # add root node to the queue\n",
    "        q_nodes.append(self.root)\n",
    "        # as long as we have nodes in the queue\n",
    "        while len(q_nodes) > 0:\n",
    "            # get first node in the queue\n",
    "            node = q_nodes.pop(0)\n",
    "\n",
    "            # check if the node is a leaf or data frame is empty\n",
    "            if len(node.data_df) < self.min_split_size or node.depth == self.depth:\n",
    "                continue\n",
    "            # calculate which attribute to split by-the one with the best ssr will be chosen\n",
    "            min_ssr, splitter, best_col = choose_column_to_split_by(data_df=node.data_df,\n",
    "                                                                    classffier_arr=node.classffier_arr)\n",
    "\n",
    "            # store the values of the attribute to split data by\n",
    "            node.column = best_col\n",
    "            node.ssr = min_ssr\n",
    "            node.splitter = splitter\n",
    "\n",
    "            # Store the data that has bigger value from the splitter\n",
    "            # and drop the current attribute from the data\n",
    "\n",
    "            left_node_df = node.data_df[node.data_df[best_col] < splitter]\n",
    "            left_node = None\n",
    "            # if exists data for the left node(bigger than the splitter)\n",
    "            if len(left_node_df) > 0:\n",
    "                # create left node and store the data in it\n",
    "                left_node = Node(\n",
    "                    data_df=left_node_df,\n",
    "                    classffier_arr=node.classffier_arr[node.data_df[best_col] < splitter],\n",
    "                    depth=node.depth + 1\n",
    "                )\n",
    "                # add the new node to the queue\n",
    "                q_nodes.append(left_node)\n",
    "\n",
    "            # Store the  data that has smaller value from the splitter\n",
    "            # and drop the current attribute from the data\n",
    "            right_node_df = node.data_df[node.data_df[best_col] >= splitter]\n",
    "            right_node = None\n",
    "            # if exists data for the right node(smaller than the splitter)\n",
    "            if len(right_node_df) > 0:\n",
    "                # create right node and store the data in it\n",
    "                right_node = Node(\n",
    "                    data_df=node.data_df[node.data_df[best_col] >= splitter],\n",
    "                    classffier_arr=node.classffier_arr[node.data_df[best_col] >= splitter],\n",
    "                    depth=node.depth + 1\n",
    "\n",
    "                )\n",
    "                # add the new node to the queue\n",
    "                q_nodes.append(right_node)\n",
    "                # store the left and right new nodes to the current node's children array\n",
    "            node.children = [left_node, right_node]\n",
    "\n",
    "    def dt_inference(self, dt: Node, data_df: pd.DataFrame, calssifier_arr):\n",
    "        sum = 0\n",
    "        # as long as we have rows in the data frame\n",
    "        for i in range(len(data_df)):\n",
    "            row = data_df.iloc[i]\n",
    "            # predict the price of the current row according to the tree's decision\n",
    "            predicted = dt.classify(row)\n",
    "            # calculate the difference between the real price and the predicted price\n",
    "            # and square it\n",
    "            sqr_residual = (calssifier_arr[i] - predicted) ** 2\n",
    "            sum += sqr_residual\n",
    "        # calculate mse by divide the total differences sum in the number of observations\n",
    "        mse = sum / len(data_df)\n",
    "        # calculate how many true predictions where made from all the data\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def dt_results(self, dt: Node, data_df: pd.DataFrame):\n",
    "        results_arr = []\n",
    "\n",
    "        # as long as we have rows in the data frame\n",
    "        for i in range(len(data_df)):\n",
    "            row = data_df.iloc[i]\n",
    "            # predict the price of the current row according to the tree's decision\n",
    "            predicted = dt.classify(row)\n",
    "            results_arr.append(predicted)\n",
    "\n",
    "        return results_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79494be7",
   "metadata": {},
   "source": [
    "Function to find the best regression tree-with the lowest mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86c4a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_reg_tree(reg_tree_A, reg_tree_B, reg_tree_C, mseA, mseB, mseC):\n",
    "    if mseA <= mseB and mseA <= mseC:\n",
    "        return reg_tree_A, mseA\n",
    "    if mseB <= mseA and mseB <= mseC:\n",
    "        return reg_tree_B, mseB\n",
    "    else:\n",
    "        return reg_tree_C, mseC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0cbaf8",
   "metadata": {},
   "source": [
    "Train run and test the regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ae2a8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n",
      "Tree with 3 was built successfully!\n",
      "The MSE of the tree is: 58316389500846.53\n",
      "Tree with 6 was built successfully!\n",
      "The MSE of the tree is: 49252961604795.86\n",
      "Tree with 9 was built successfully!\n",
      "The MSE of the tree is: 57069055678763.38\n",
      "The best regression Tree is has 6 depth, with mse: 49252961604795.86\n",
      "run time: 0:00:05.825498\n",
      "Test:\n",
      "The MSE of the tree is: 67386316078160.95\n"
     ]
    }
   ],
   "source": [
    "# get the path to the folder of the script\n",
    "folder_path = os.getcwd()\n",
    "# store csv file name as a string\n",
    "csv_name = \"data.csv\"\n",
    "# create the path and store it\n",
    "csv_path = os.path.join(folder_path, csv_name)\n",
    "# Read data from csv file and store in real_estate\n",
    "real_estate = pd.read_csv(csv_path, index_col=[0])\n",
    "\n",
    "# store area_type column as classifier\n",
    "classfier_arr = np.array(real_estate['price in rupees'])\n",
    "# store data without the classifier\n",
    "real_estate_features = real_estate.drop('price in rupees', axis=1)\n",
    "real_estate_features.loc[(real_estate_features.area_type == 'B'), \"area_type\"] = 0\n",
    "real_estate_features.loc[(real_estate_features.area_type == 'P'), \"area_type\"] = 1\n",
    "real_estate_features['area_type'] = pd.to_numeric(real_estate_features['area_type'])\n",
    "# store train data\n",
    "train_df = real_estate_features.iloc[:8040]\n",
    "# store the classifier for train data\n",
    "train_price_arr = classfier_arr[:8040]\n",
    "# store the validation data\n",
    "validation_df = real_estate_features.iloc[8041:10050]\n",
    "# store the validation classifer\n",
    "validation_cls_arr = classfier_arr[8041:10050]\n",
    "\n",
    "print(\"Validation:\")\n",
    "###########trail 1 with minimum 3 observations and maximu depth=3\n",
    "# build decision tree based on training data\n",
    "start_time_A = datetime.now()\n",
    "reg_tree_A = Regression_tree(3, 3)\n",
    "reg_tree_A.dt_trainer(train_df, train_price_arr)\n",
    "print(\"Tree with\", reg_tree_A.depth, \"was built successfully!\")\n",
    "mseA = reg_tree_A.dt_inference(reg_tree_A.root, validation_df, validation_cls_arr)\n",
    "print(\"The MSE of the tree is:\", mseA)\n",
    "end_time_A = datetime.now()\n",
    "##########################################################\n",
    "\n",
    "###########trail 2 with minimum 3 observations and maximu depth=6\n",
    "# build decision tree based on training data\n",
    "start_time_B = datetime.now()\n",
    "reg_tree_B = Regression_tree(3, 6)\n",
    "# store the validation data\n",
    "reg_tree_B.dt_trainer(train_df, train_price_arr)\n",
    "print(\"Tree with\", reg_tree_B.depth, \"was built successfully!\")\n",
    "mseB = reg_tree_B.dt_inference(reg_tree_B.root, validation_df, validation_cls_arr)\n",
    "print(\"The MSE of the tree is:\", mseB)\n",
    "end_time_B = datetime.now()\n",
    "##########################################################\n",
    "\n",
    "###########trail 3 with minimum 3 observations and maximu depth=9\n",
    "# build decision tree based on training data\n",
    "start_time_C = datetime.now()\n",
    "reg_tree_C = Regression_tree(3, 9)\n",
    "# store the validation data\n",
    "reg_tree_C.dt_trainer(train_df, train_price_arr)\n",
    "print(\"Tree with\", reg_tree_C.depth, \"was built successfully!\")\n",
    "mseC = reg_tree_C.dt_inference(reg_tree_C.root, validation_df, validation_cls_arr)\n",
    "print(\"The MSE of the tree is:\", mseC)\n",
    "end_time_C = datetime.now()\n",
    "##########################################################\n",
    "\n",
    "best_tree, best_mse = best_reg_tree(reg_tree_A, reg_tree_B, reg_tree_C, mseA, mseB, mseC)\n",
    "print(\"The best regression Tree is has\", best_tree.depth, \"depth, with mse:\", best_mse)\n",
    "if best_tree == reg_tree_A:\n",
    "    print('run time: {}'.format(end_time_A - start_time_A))\n",
    "    best_runtime=end_time_A - start_time_A\n",
    "if best_tree == reg_tree_B:\n",
    "    print('run time: {}'.format(end_time_B - start_time_B))\n",
    "    best_runtime=end_time_B - start_time_B\n",
    "if best_tree == reg_tree_C:\n",
    "    print('run time: {}'.format(end_time_C - start_time_C))\n",
    "    best_runtime=end_time_C - start_time_C\n",
    "\n",
    "print(\"Test:\")\n",
    "validation_results = best_tree.dt_results(best_tree.root, validation_df)\n",
    "# store the test data\n",
    "test_df = real_estate_features.iloc[10051:]\n",
    "# store the test classifer\n",
    "test_price_arr = classfier_arr[10051:]\n",
    "test_mse = best_tree.dt_inference(best_tree.root, test_df, test_price_arr)\n",
    "print(\"The MSE of the tree is:\", test_mse)\n",
    "test_results = best_tree.dt_results(best_tree.root, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720da808",
   "metadata": {},
   "source": [
    "## Section c:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d76b3",
   "metadata": {},
   "source": [
    "define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b9307f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "# define parameter grid\n",
    "parameters_grid = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'min_samples_leaf': [20,2,3]\n",
    "}\n",
    "\n",
    "# define grid search\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=parameters_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3bd1ca",
   "metadata": {},
   "source": [
    "fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d670f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor(max_depth=9, min_samples_leaf=3)\n"
     ]
    }
   ],
   "source": [
    "# fit estimator\n",
    "sk_reg_tree_start=datetime.now()\n",
    "grid_search.fit(train_df, train_price_arr)\n",
    "sk_reg_tree_end=datetime.now()\n",
    "\n",
    "# get best estimator\n",
    "best = grid_search.best_estimator_\n",
    "#print best max_depth\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b7b48",
   "metadata": {},
   "source": [
    "predict the area type for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c75965d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = best.predict(validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d752a0",
   "metadata": {},
   "source": [
    "calculate mse for validation data and print it+compare to the mse of the presonal regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e4a6790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklrean tree:\n",
      "                                  MSE\n",
      "Decision Tree Regressor  4.468983e+13\n",
      "run time: 0:00:01.094749\n",
      "personal tree with depth: 6\n",
      "MSE: 49252961604795.86\n",
      "run time: 0:00:05.825498\n"
     ]
    }
   ],
   "source": [
    "#calculate accuracy for validation data\n",
    "MSE = round(mean_squared_error(validation_cls_arr,predictions), 3)\n",
    "\n",
    "df = pd.DataFrame([MSE])\n",
    "df = df.rename(index={0: 'Decision Tree Regressor'}, columns={0: 'MSE'})\n",
    "print(\"sklrean tree:\")\n",
    "print(df)\n",
    "print('run time: {}'.format(sk_reg_tree_end-sk_reg_tree_start))\n",
    "print(\"personal tree with depth:\",best_tree.depth)\n",
    "print('MSE:',best_mse)\n",
    "print('run time: {}'.format(best_runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d01db",
   "metadata": {},
   "source": [
    "predict the area type for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aee171dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = best.predict(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516b0fe",
   "metadata": {},
   "source": [
    "calculate mse for the test data and print it+compare to the mse of the presonal regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d665592c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklrean tree:\n",
      "                                  MSE\n",
      "Decision Tree Regressor  6.027681e+13\n",
      "personal tree with depth: 6\n",
      "MSE: 67386316078160.95\n"
     ]
    }
   ],
   "source": [
    "#calculate accuracy for validation data\n",
    "MSE = round(mean_squared_error(test_price_arr,predictions), 3)\n",
    "\n",
    "df = pd.DataFrame([MSE])\n",
    "df = df.rename(index={0: 'Decision Tree Regressor'}, columns={0: 'MSE'})\n",
    "print(\"sklrean tree:\")\n",
    "print(df)\n",
    "print(\"personal tree with depth:\",best_tree.depth)\n",
    "print('MSE:',test_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8630a7d3",
   "metadata": {},
   "source": [
    "Sklearn algorithm for regression tree has 12% lower MSE from our regression tree algorithm.\n",
    "In addition sklearn algorith is much more faster than our algorithm.\n",
    "We estimate the reason for that is that we used some while and for loops, while sklearn used more efficient methods we could not think of.\n",
    "In addition they use gridsearch to tune the best hyper-parameters, while we don't, which makes their algorithm more accurate and much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdb10e",
   "metadata": {},
   "source": [
    "## Adaboost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f50938",
   "metadata": {},
   "source": [
    "\n",
    "Function to calculate Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3cae8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(classffier_arr: np.ndarray):\n",
    "    # determine which unique values the classffier_arry have and count how many from each class\n",
    "    values, counts = np.unique(classffier_arr, return_counts=True)\n",
    "    # calculate how many values all and all in the classfier\n",
    "    S = len(classffier_arr)\n",
    "    # calculate gini index equation\n",
    "    gini_idx = 1 - ((counts / S) ** 2).sum()\n",
    "\n",
    "    return gini_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51b84b",
   "metadata": {},
   "source": [
    "Function to calculate the total Gini of the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a9f3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_gini(attr_array: np.ndarray, splitter: float, classffier_arr: np.ndarray):\n",
    "    # store all values above the splitter value\n",
    "    above_splitter = classffier_arr[attr_array >= splitter]\n",
    "    # store all values below the splitter value\n",
    "    below_splitter = classffier_arr[attr_array < splitter]\n",
    "\n",
    "    # calculate gini index for both groups\n",
    "    above_splitter_gini = gini_index(classffier_arr=above_splitter)\n",
    "    below_splitter_gini = gini_index(classffier_arr=below_splitter)\n",
    "    # calculate how many values in the column\n",
    "    S = len(classffier_arr)\n",
    "    # calculate the totla gini of the column, based on the current splitter\n",
    "    total = (len(above_splitter) / S) * above_splitter_gini + (len(below_splitter) / S) * below_splitter_gini\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ce388",
   "metadata": {},
   "source": [
    "Function to calculate the best Gini Index for an attribute\n",
    "<br>\n",
    "Returns the best Gini and the best splitter of the obserations to get this Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aefa57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_gini_for_attr(attr_array, classfier_arr):\n",
    "    # store the unique values of the data in a sorted array\n",
    "    unique_attr_array = np.unique(attr_array)\n",
    "    min_gini = 1\n",
    "    min_splitter = 0\n",
    "\n",
    "    # for every mean between 2 adjacent values, calculate total gini\n",
    "    for i in range((len(unique_attr_array) - 1)):\n",
    "        splitter = float(np.mean(unique_attr_array[i:i + 2]))\n",
    "        temp_gini = total_gini(attr_array, splitter, classfier_arr)\n",
    "\n",
    "        # if its the best total gini store it as min_gini\n",
    "        if temp_gini < min_gini:\n",
    "            min_gini = temp_gini\n",
    "            min_splitter = splitter\n",
    "\n",
    "    return min_gini, min_splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cff109",
   "metadata": {},
   "source": [
    "Function to choose the best attribute to split by the data\n",
    "<br>\n",
    "The attribute with the minimum Gini index will be chosen\n",
    "<br>\n",
    "The function returns:\n",
    "<br>\n",
    "min_gini- the minimum Gini of the attribute to split by\n",
    "<br>\n",
    "splitter-the value to split by the data\n",
    "<br>\n",
    "best_col-the name to the attribute to split the data by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66dd789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_column_to_split_by(data_df: pd.DataFrame, classffier_arr: np.ndarray):\n",
    "    best_col = ''\n",
    "    splitter = 0\n",
    "    min_gini = 1.1\n",
    "    # for every attribute in the data frame\n",
    "    for column in data_df.columns.values:\n",
    "        # calculate the best gini for the current attribute\n",
    "        temp_gini, temp_splitter = best_gini_for_attr(data_df[column].values, classfier_arr=classffier_arr)\n",
    "        # if it is the best total gini-store it as min gini,\n",
    "        # the best splitter for this attribute and the attribute's name\n",
    "        if temp_gini < min_gini:\n",
    "            min_gini = temp_gini\n",
    "            splitter = temp_splitter\n",
    "            best_col = column\n",
    "\n",
    "    return min_gini, splitter, best_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f5701",
   "metadata": {},
   "source": [
    "Class Node: defining the nodes of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb035305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_df: pd.DataFrame,\n",
    "            classffier_arr: np.ndarray,\n",
    "            depth\n",
    "    ):\n",
    "        self.data_df = data_df\n",
    "        self.classffier_arr = classffier_arr\n",
    "        self.depth = depth\n",
    "        self.column = None\n",
    "        self.gini = None\n",
    "        self.splitter = None\n",
    "        # array for left and right children\n",
    "        self.children = []\n",
    "        # determine which unique values the classffier_arry have and count how many from each class\n",
    "        values, counts = np.unique(self.classffier_arr, return_counts=True)\n",
    "        # store and array of the unique values in the classfier\n",
    "        self.values_names = values\n",
    "        # store the count of each class\n",
    "        self.values = counts\n",
    "        # store the class with the most values\n",
    "        self.cls = values[np.argmax(counts)]\n",
    "\n",
    "    # if all the data is form 1 class\n",
    "    def perfectly_classified(self):\n",
    "        return len(self.values) == 1\n",
    "\n",
    "    # gets only one row from the data\n",
    "    def classify(self, data_row):\n",
    "        # if the node is a leaf than classify\n",
    "        # the current row as the majority class of the current node\n",
    "        if len(self.children) == 0:\n",
    "            return self.cls\n",
    "        # store the current value of the column we are currently splitting by\n",
    "        current_value = data_row[self.column]\n",
    "\n",
    "        # if the current value is smaller than the node's splitter\n",
    "        # go to the left child to classify\n",
    "        # return the result of the left child\n",
    "        if current_value < self.splitter and self.children[0] is not None:\n",
    "            return self.children[0].classify(data_row)\n",
    "        # if the current value is bigger or equals than the node's splitter\n",
    "        # go to the right child to classify\n",
    "        # return the result of the right child\n",
    "        elif current_value >= self.splitter and self.children[1] is not None:\n",
    "            return self.children[1].classify(data_row)\n",
    "        else:\n",
    "            # return the class of this node\n",
    "            return self.cls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3a87e",
   "metadata": {},
   "source": [
    "Defina a class of a stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2cd85b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stump:\n",
    "    def __init__(\n",
    "            self, data_df: pd.DataFrame,\n",
    "            classffier_arr: np.ndarray,\n",
    "    ):\n",
    "        self.data_df = data_df\n",
    "        self.classffier_arr = classffier_arr\n",
    "        self.childern = []\n",
    "        self.root = None\n",
    "        # stores the weight for each observation in the data frame\n",
    "        self.weight_array = np.array([])\n",
    "        # stores the stump's weight\n",
    "        self.weight = None\n",
    "\n",
    "    # function to fill the stumps with it's root and 2 leafs\n",
    "    # and his properties\n",
    "    def stump_trainer(self, data_df: pd.DataFrame, classffier_arr: np.ndarray):\n",
    "        # create root node\n",
    "        root_node = Node(data_df=data_df, classffier_arr=classffier_arr, depth=0)\n",
    "        # calculate which attribute to split by-the one with the best gini will be chosen\n",
    "        min_gini, splitter, best_col = choose_column_to_split_by(data_df=root_node.data_df,\n",
    "                                                                 classffier_arr=root_node.classffier_arr)\n",
    "        # store the values of the attribute to split data by\n",
    "        root_node.column = best_col\n",
    "        root_node.gini = min_gini\n",
    "        root_node.splitter = splitter\n",
    "\n",
    "        # The start weight for every observation will be\n",
    "        # 1/the number of the observations in the data frame\n",
    "        start_weight = 1 / len(data_df)\n",
    "        # store in weight_array the start weight for every observation\n",
    "        self.weight_array = np.full(len(data_df), start_weight)\n",
    "\n",
    "        # Store the data that has smaller value from the splitter\n",
    "        # and drop the current attribute from the data\n",
    "        left_node_df = root_node.data_df[root_node.data_df[best_col] < splitter]\n",
    "        left_node = None\n",
    "        # if exists data for the left node(bigger than the splitter)\n",
    "        if len(left_node_df) > 0:\n",
    "            # create left node and store the data in it\n",
    "            left_node = Node(\n",
    "                data_df=left_node_df,\n",
    "                classffier_arr=root_node.classffier_arr[root_node.data_df[best_col] < splitter],\n",
    "                depth=root_node.depth + 1\n",
    "            )\n",
    "\n",
    "        # Store the  data that has bigger or equal value from the splitter\n",
    "        right_node_df = root_node.data_df[root_node.data_df[best_col] >= splitter]\n",
    "        right_node = None\n",
    "        # if exists data for the right node(smaller than the splitter)\n",
    "        if len(right_node_df) > 0:\n",
    "            # create right node and store the data in it\n",
    "            right_node = Node(\n",
    "                data_df=root_node.data_df[root_node.data_df[best_col] >= splitter],\n",
    "                classffier_arr=root_node.classffier_arr[root_node.data_df[best_col] >= splitter],\n",
    "                depth=root_node.depth + 1\n",
    "            )\n",
    "        # store the left and right new nodes to the current node's children array\n",
    "        root_node.children = [left_node, right_node]\n",
    "        return root_node\n",
    "\n",
    "    # function to calculate the total error of the stump\n",
    "    def total_error(self):\n",
    "        # initialize the number of errors to 0\n",
    "        error = 0\n",
    "        # store the stump's data frame in data_df\n",
    "        data_df = self.data_df\n",
    "        # store the stump's classffier_arr in classfier_arr\n",
    "        classifier_arr = self.classffier_arr\n",
    "        # as long as we have rows in the data frame\n",
    "        for i in range(len(data_df)):\n",
    "            row = data_df.iloc[i]\n",
    "            # predict the class of the current row according to the stump's decision\n",
    "            predicted = self.root.classify(row)\n",
    "            # store the real class of the current row\n",
    "            real = classifier_arr[i]\n",
    "\n",
    "            # if the predition was wrong- count it\n",
    "            if real != predicted:\n",
    "                error += 1\n",
    "        # return the total error for this stump\n",
    "        return error / len(data_df)\n",
    "\n",
    "    # function to update the weight of the data frame's observations\n",
    "    def change_weight(self):\n",
    "        # store the stump's data frame in data_df\n",
    "        data_df = self.data_df\n",
    "        # store the stump's classffier_arr in classfier_arr\n",
    "        classifier_arr = self.classffier_arr\n",
    "        # store the new weight for an observation correctly classified\n",
    "        # according to the equation: new_weight=sample_weight*e**(-stump_weight)\n",
    "        right_weight = self.weight_array[0] * math.e ** (-self.weight)\n",
    "        # store the new weight for an observation incorrectly classified\n",
    "        # according to the equation: new_weight=sample_weight*e**stump_weight\n",
    "        wrong_weight = self.weight_array[0] * math.e ** self.weight\n",
    "        # as long as we have rows in the data frame\n",
    "        for i in range(len(data_df)):\n",
    "            row = data_df.iloc[i]\n",
    "            # predict the class of the current row according to the stump's decision\n",
    "            predicted = self.root.classify(row)\n",
    "            # store the real class of the current row\n",
    "            real = classifier_arr[i]\n",
    "\n",
    "            # if the prediction was wrong- increase weight\n",
    "            if real != predicted:\n",
    "                self.weight_array[i] = wrong_weight\n",
    "            # if the prediction was right-decrease the weight\n",
    "            else:\n",
    "                self.weight_array[i] = right_weight\n",
    "            # calculate the sum of the weights of all observations\n",
    "        sum_weight = self.weight_array.sum()\n",
    "        # normalize the weight of every observation by\n",
    "        # divding the current observation's weight by the sum of the weights of all observations\n",
    "        for i in range(len(self.weight_array)):\n",
    "            self.weight_array[i] = self.weight_array[i] / sum_weight\n",
    "\n",
    "    # function to create a new dataframe and his classifer to create from them a new stump\n",
    "    def create_weighted_df(self):\n",
    "        # Intialize an array to store the weights ranges\n",
    "        ranges_array = np.array([])\n",
    "        weight_sum = 0\n",
    "        # for every weight of an observation\n",
    "        for curr_weight in self.weight_array:\n",
    "            # add the weight of the current observation to weight_sum\n",
    "            weight_sum = weight_sum + curr_weight\n",
    "            # append the new sum to the weight ranges array\n",
    "            ranges_array = np.append(ranges_array, weight_sum)\n",
    "        # create a new data frame with the same columns as the given data frame\n",
    "        new_df = pd.DataFrame(columns=self.data_df.columns)\n",
    "        # create a new empty classfier\n",
    "        new_classfier = np.array([])\n",
    "        # for every observation in the data frame\n",
    "        for i in range(len(self.data_df)):\n",
    "            # use a random number between 0 and 1\n",
    "            # and save the index that should be for the random number if it was in the\n",
    "            # weights array\n",
    "            index = bisect.bisect_right(ranges_array, random.random())\n",
    "            # add the index of this row to the new data frame\n",
    "            # this step will create a data frame with the observations that have more weight\n",
    "            # more times\n",
    "            new_df = new_df.append(self.data_df.iloc[index], ignore_index=True)\n",
    "            # add the class of the selected observation to the classifier array\n",
    "            new_classfier = np.append(new_classfier, self.classffier_arr[index])\n",
    "        # return the new data frame and his classifier to create a new stump from them\n",
    "        return new_df, new_classfier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17974b8",
   "metadata": {},
   "source": [
    "Define a class of adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9677cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_stumps\n",
    "    ):\n",
    "        # hyper parameter for the number of stumps\n",
    "        self.max_stumps = max_stumps\n",
    "        # an array to store the stumps of the booster\n",
    "        self.stumps_array = []\n",
    "\n",
    "    # function to create the stumps of the booster and store them in the booster's stumps array\n",
    "    def adaboost_trainer(self, data_df: pd.DataFrame, classffier_arr: np.ndarray):\n",
    "        # create the selected number of stumps\n",
    "        for i in range(1, self.max_stumps):\n",
    "            # create a new empty stump\n",
    "            temp_stump = Stump(data_df, classffier_arr)\n",
    "            # Create the stump's root and nodes\n",
    "            temp_stump.root = temp_stump.stump_trainer(data_df, classffier_arr)\n",
    "            # calculate the total error of the stump and store it in the stump\n",
    "            temp_error = temp_stump.total_error()\n",
    "            # calculate the weight of the sutmp according to totla error and store it in the stump\n",
    "            temp_stump.weight = 0.5 * math.log((1 - temp_error) / temp_error)\n",
    "            # update the weight of every observation in the stump\n",
    "            temp_stump.change_weight()\n",
    "            # add the new stump to the booster's array of stumps\n",
    "            self.stumps_array.append(temp_stump)\n",
    "            # create a new data frame and classffier to create a new stump\n",
    "            # the new data frame will contain the observations with bigger weight\n",
    "            # more times\n",
    "            data_df, classffier_arr = temp_stump.create_weighted_df()\n",
    "\n",
    "    # function to predict the class of the row according to the weights of the stumps\n",
    "    def predict(self, data_row):\n",
    "        # the sum of weights of stumps that chose 'B' class\n",
    "        btype_sum = 0\n",
    "        # the sum of weights of stumps that chose 'P' class\n",
    "        ptype_sum = 0\n",
    "        # f or every stump in the booster\n",
    "        for stump in self.stumps_array:\n",
    "            # predict the class with the current stump\n",
    "            predicted = stump.root.classify(data_row)\n",
    "            # add the weight of the stump to the sum of the result class\n",
    "            if predicted == 'B':\n",
    "                btype_sum += stump.weight\n",
    "            else:\n",
    "                ptype_sum += stump.weight\n",
    "        # predict the class of the observation according to the class that has more weight\n",
    "        # from all of the stumps\n",
    "        if btype_sum <= ptype_sum:\n",
    "            return 'P'\n",
    "        return 'B'\n",
    "\n",
    "    # function to predict the class of every observation and calculate the accuracy of the booster\n",
    "    def inference(self, data_df: pd.DataFrame, calssifier_arr):\n",
    "        correct = 0\n",
    "        # as long as we have rows in the data frame\n",
    "        for i in range(len(data_df)):\n",
    "            row = data_df.iloc[i]\n",
    "            # predict the class of the current row according to the tree's decision\n",
    "            predicted = self.predict(row)\n",
    "            # store the real class of the current row\n",
    "            real = calssifier_arr[i]\n",
    "\n",
    "            # if the prediction was correct- count it\n",
    "            if real == predicted:\n",
    "                correct += 1\n",
    "\n",
    "        # calculate how many true predictions where made from all the data\n",
    "        accuracy = correct / len(data_df)\n",
    "        return accuracy\n",
    "\n",
    "    # function to return an array of the predictions of every observation\n",
    "    def results(self, data_df: pd.DataFrame):\n",
    "        results_arr = []\n",
    "        # as long as we have rows in the data frame\n",
    "        for i in range(len(data_df)):\n",
    "            row = data_df.iloc[i]\n",
    "            # predict the class of the current row according to the booster's decision\n",
    "            predicted = self.predict(row)\n",
    "            results_arr.append(predicted)\n",
    "\n",
    "        return results_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab67b899",
   "metadata": {},
   "source": [
    "Function to choose the best booster- with the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f69bb83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_booster(booster_A, booster_B, booster_C, accA, accB, accC):\n",
    "    if accA >= accB and accA >= accC:\n",
    "        return booster_A, accA\n",
    "    if accB >= accA and accB >= accC:\n",
    "        return booster_B, accB\n",
    "    else:\n",
    "        return booster_C, accC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29be787",
   "metadata": {},
   "source": [
    "Train, validate and test the booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c403dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n",
      "Booster with 5 stumps was built successfully!\n",
      "The accuracy of the Booster is: 0.8735689397710303\n",
      "Booster with 8 stumps was built successfully!\n",
      "The accuracy of the Booster is: 0.8735689397710303\n",
      "Booster with 10 stumps was built successfully!\n",
      "The accuracy of the Booster is: 0.8780487804878049\n",
      "The best Booster has 10 stumps, with accuracy: 87.805%\n",
      "run time: 0:02:23.501400\n",
      "Test:\n",
      "The accuracy of the booster is: 87.500%\n"
     ]
    }
   ],
   "source": [
    "# get the path to the folder of the script\n",
    "folder_path = os.getcwd()\n",
    "# store csv file name as a string\n",
    "csv_name = \"data.csv\"\n",
    "# create the path and store it\n",
    "csv_path = os.path.join(folder_path, csv_name)\n",
    "# Read data from csv file and store in real_estate\n",
    "real_estate = pd.read_csv(csv_path, index_col=[0])\n",
    "                             \n",
    "# store area_type column as classfier\n",
    "classfier_arr = np.array(real_estate['area_type'])\n",
    "# store data without the classfier\n",
    "real_estate_features = real_estate.drop('area_type', axis=1)\n",
    "# store train data\n",
    "train_df = real_estate_features.iloc[:8040]\n",
    "# store the classfier for train data\n",
    "train_cls_arr = classfier_arr[:8040]\n",
    "# store the validation data\n",
    "validation_df = real_estate_features.iloc[8041:10050]\n",
    "# store the validation classifer\n",
    "validation_cls_arr = classfier_arr[8041:10050]\n",
    "print(\"Validation:\")\n",
    "\n",
    "##############trail 1 with 5 stumps ##########\n",
    "start_time_A = datetime.now()\n",
    "# build Booster with 5 stumps\n",
    "booster_A = Adaboost(5)\n",
    "booster_A.adaboost_trainer(train_df, train_cls_arr)\n",
    "print(\"Booster with\", booster_A.max_stumps, \"stumps was built successfully!\")\n",
    "accA = booster_A.inference(validation_df, validation_cls_arr)\n",
    "print(\"The accuracy of the Booster is:\", accA)\n",
    "end_time_A = datetime.now()\n",
    "##########################################################\n",
    "\n",
    "##############trail 2 with 8 stumps ##########\n",
    "start_time_B = datetime.now()\n",
    "# build Booster with 8 stumps\n",
    "booster_B = Adaboost(8)\n",
    "booster_B.adaboost_trainer(train_df, train_cls_arr)\n",
    "print(\"Booster with\", booster_B.max_stumps, \"stumps was built successfully!\")\n",
    "accB = booster_B.inference(validation_df, validation_cls_arr)\n",
    "print(\"The accuracy of the Booster is:\", accA)\n",
    "end_time_B = datetime.now()\n",
    "##########################################################\n",
    "\n",
    "##############trail 1 with 10 stumps ##########\n",
    "start_time_C = datetime.now()\n",
    "# build Booster with 10 stumps\n",
    "booster_C = Adaboost(10)\n",
    "booster_C.adaboost_trainer(train_df, train_cls_arr)\n",
    "print(\"Booster with\", booster_C.max_stumps, \"stumps was built successfully!\")\n",
    "accC = booster_C.inference(validation_df, validation_cls_arr)\n",
    "print(\"The accuracy of the Booster is:\", accC)\n",
    "end_time_C = datetime.now()\n",
    "##########################################################\n",
    "\n",
    "best_adaboost, best_accuracy = best_booster(booster_A, booster_B, booster_C, accA, accB, accC)\n",
    "best_accuracy = f\"{best_accuracy:.3%}\"\n",
    "print(\"The best Booster has\", best_adaboost.max_stumps, \"stumps, with accuracy:\", best_accuracy)\n",
    "if best_adaboost == booster_A:\n",
    "    print('run time: {}'.format(end_time_A - start_time_A))\n",
    "    best_runtime=end_time_A - start_time_A\n",
    "if best_adaboost == booster_B:\n",
    "    print('run time: {}'.format(end_time_B - start_time_B))\n",
    "    best_runtime=end_time_B - start_time_B\n",
    "if best_adaboost == booster_C:\n",
    "    print('run time: {}'.format(end_time_C - start_time_C))\n",
    "    best_runtime=end_time_C - start_time_C\n",
    "\n",
    "validation_results = best_adaboost.results(validation_df)\n",
    "# store the test data\n",
    "test_df = real_estate_features.iloc[10051:]\n",
    "# store the test classifer\n",
    "test_cls_arr = classfier_arr[10051:]\n",
    "# calculate accuracy of the test data based on the booster\n",
    "best_accuracy = best_adaboost.inference(test_df, test_cls_arr)\n",
    "best_accuracy = f\"{best_accuracy:.3%}\"\n",
    "print(\"Test:\")\n",
    "print(\"The accuracy of the booster is:\", best_accuracy)\n",
    "# store test results\n",
    "test_results = best_adaboost.results(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393b3a4",
   "metadata": {},
   "source": [
    "## Section C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc193f",
   "metadata": {},
   "source": [
    "Import relevant libraries from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51e5b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "\n",
    "# gradient boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# model building\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b08eaf",
   "metadata": {},
   "source": [
    "define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "869d6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "ab = AdaBoostClassifier()\n",
    "\n",
    "# define parameter grid\n",
    "parameters_grid = {\n",
    "    'n_estimators': [5, 8,10]\n",
    "}\n",
    "\n",
    "# define grid search\n",
    "grid_search = GridSearchCV(estimator=ab, param_grid=parameters_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63fce49",
   "metadata": {},
   "source": [
    "fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "daa9d3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Selected Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_estimators</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Selected Value\n",
       "n_estimators              10"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit estimator\n",
    "adaboost_start_time=datetime.now()\n",
    "grid_search.fit(validation_df, validation_cls_arr)\n",
    "adaboost_end_time=datetime.now()\n",
    "\n",
    "# get best estimator\n",
    "best = grid_search.best_estimator_\n",
    "pd.DataFrame.from_dict(grid_search.best_params_, orient='index', columns=['Selected Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568629f",
   "metadata": {},
   "source": [
    "predict the area type of the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f4cd407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=best.predict(validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c70777",
   "metadata": {},
   "source": [
    "calculate accuracy for validation data and print it+compare to the mse of the presonal regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f5e728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn Adaboost:\n",
      "                     Accuracy\n",
      "AdaBoost Classifier     0.884\n",
      "run time: 0:00:00.816227\n",
      "personal booster with: 10 stumps:\n",
      "Accuracy: 87.500%\n",
      "run time: 0:02:23.501400\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "acc = round(accuracy_score(validation_cls_arr, predictions), 3)\n",
    "\n",
    "df = pd.DataFrame([acc])\n",
    "df = df.rename(index={0: 'AdaBoost Classifier'}, columns={0: 'Accuracy'})\n",
    "print(\"sklearn Adaboost:\")\n",
    "print(df)\n",
    "print('run time: {}'.format(adaboost_end_time-adaboost_start_time))\n",
    "print(\"personal booster with:\",best_adaboost.max_stumps,\"stumps:\")\n",
    "print('Accuracy:',best_accuracy)\n",
    "print('run time: {}'.format(best_runtime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec7fd95",
   "metadata": {},
   "source": [
    "predict the area type of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a5360be",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=best.predict(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597f9260",
   "metadata": {},
   "source": [
    "calculate accuracy for test data and print it+compare to the accuracy of the presonal booster classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "17a920aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklrean Adaboost:\n",
      "                     Accuracy\n",
      "Adaboost Classifier     0.885\n",
      "personal Adaboost:\n",
      "87.500%\n"
     ]
    }
   ],
   "source": [
    "#calculate accuracy for validation data\n",
    "acc = round(accuracy_score(test_cls_arr,predictions), 3)\n",
    "\n",
    "df = pd.DataFrame([acc])\n",
    "df = df.rename(index={0: 'Adaboost Classifier'}, columns={0: 'Accuracy'})\n",
    "print(\"sklrean Adaboost:\")\n",
    "print(df)\n",
    "print(\"personal Adaboost:\")\n",
    "print(best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6cf0dd",
   "metadata": {},
   "source": [
    "Our Adaboost is more accurate then sklearn's adaboost, but much slower.\n",
    "We estimate the reason for that is that we used some for and while loops in our code, while sklearn used more efficient methods we could not think of.\n",
    "In addition they use gridsearch to tune the best hyper-parameters, while we don't, which makes their algorithm more accurate and much faster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
